{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72c18f9",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Now that we are able to detect the HexBugs using the `Yolo` model we trained, we can crop the specific regions containing the targets. But there is a catch!\n",
    "Bounded rectangles doesn't have same shape! We need them to have same shape to feed them to our CNN model. To do so, we analyze the boundary box and crop the image in a way that it has a shape of `(300, 300)`. Since the predicted coordinations in the cropped image need to be scaled, we keep track of the boundary box's coordinations as well, and save them in a `data.csv` file to work with later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0da2e",
   "metadata": {},
   "source": [
    "### Imports\n",
    "* `os` is essential to manage directories.\n",
    "* `json` is needed to open `traco` json files, which contains our annotations.\n",
    "* `torch` is required as we use `ultralytics`, which is implemented in `Pytorch`.\n",
    "* `numpy` is used to convert images to arrays and vice versa.\n",
    "* `cv2` is essential to read and show images.\n",
    "* `pandas` is required to work with dataframes. e.g. the `data.csv` file we created before.\n",
    "* `time` is required to track time.\n",
    "* `ultralytics` is needed to work with our `Yolo` model.\n",
    "* `supervision` is required to work with `Roboflow` smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c2a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231f6af",
   "metadata": {},
   "source": [
    "### Trained model location\n",
    "Here we put the location to the `best.pt` model we trained before. it must be at `runs/detect/train/weights/best.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30513f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_model = 'runs/detect/train/weights/best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133a2bd",
   "metadata": {},
   "source": [
    "### Creating ObjectDetection class\n",
    "To work easier with the trained model, we can create a class to do so.\n",
    "#### Note Please\n",
    "The original idea for this is for [Nicolai Høirup Nielsen](https://github.com/niconielsen32/YOLOv8-Class/blob/main/YOLOv8InferenceClass.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7b8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetection:\n",
    "    # The class takes only the capture index. Index 0 is the main camera of the device and \n",
    "    # was used to test real-time capture.\n",
    "    def __init__(self, capture_index):\n",
    "\n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        # Checks if the device supports `cuda`\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "        self.model = self.load_model()\n",
    "\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "\n",
    "        print(self.CLASS_NAMES_DICT)\n",
    "        \n",
    "        # Uses the supervision library to plot bounding boxes.\n",
    "        self.box_annotator = sv.BoxAnnotator(sv.ColorPalette.default(), thickness=2, text_thickness=1, text_scale=1)\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the trained Yolo model.\n",
    "        \n",
    "        @param: None\n",
    "        @return: Model model\n",
    "        \"\"\"\n",
    "        model = YOLO(bug_model)\n",
    "        model.fuse()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        \"\"\"\n",
    "        Predicts the results in the given image frame.\n",
    "        \n",
    "        @param: Image frame\n",
    "        @return: numpy.ndarray results. contains `xyxy`, `confidence`, `class_ids` of detected objects.\n",
    "        \"\"\"\n",
    "        results = self.model(frame)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        \"\"\"\n",
    "        Plots the bounding boxes around the detected results. It contains the class_id and the confidence score.\n",
    "        \n",
    "        @param: numpy.ndarray results. Is the output from self.predict()\n",
    "        @return: Image frame. The results are all labeled in the original image.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Creating lists to track the results output\n",
    "        xyxys = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        # Extract detections for HexBug (class_id 0)\n",
    "        for result in results:\n",
    "            # Extracting the `boxes` data, which contains the bounding boxes' position.\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            # Checks if the the detector couldn't find any results. In that case it will return the original image.\n",
    "            if len(boxes) > 0:\n",
    "                # There were at least one result found.\n",
    "                class_id = boxes.cls[0]\n",
    "                conf = boxes.conf[0]\n",
    "                xyxy = boxes.xyxy[0]\n",
    "                \n",
    "                xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "                confidences.append(result.boxes.conf.cpu().numpy())\n",
    "                class_ids.append(result.boxes.cls.cpu().numpy().astype(int))\n",
    "\n",
    "        # Setup detections for visualization\n",
    "        detections = sv.Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n",
    "        )\n",
    "\n",
    "        # Format custom labels\n",
    "        self.labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "                       for _, _, confidence, class_id, tracker_id\n",
    "                       in detections]\n",
    "\n",
    "        # Annotate and display frame\n",
    "        frame = self.box_annotator.annotate(scene=frame, detections=detections, labels=self.labels)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f46774",
   "metadata": {},
   "source": [
    "### Creating a class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657d1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n",
      "{0: '-', 1: 'HexBug'}\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection(capture_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e634959",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac260006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes sure that the temporary directory `cropped_bugs` doesn't contain any items.\n",
    "if not os.path.exists('cropped_bugs'):\n",
    "    os.mkdir('cropped_bugs')\n",
    "\n",
    "# Creates a pandas dataframe which contains `frame_ID`, `file_path`, `(x, y) coordination of the Hexbug`, \n",
    "# and `(x, y) of its bounding box`\n",
    "data = pd.DataFrame(columns=['CroppedHexBugCoordinationX', 'CroppedHexBugCoordinationY',\n",
    "                             'OriginalBoxCoordinationX1', 'OriginalBoxCoordinationY1',\n",
    "                             'Path', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9452bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 53/53 [02:05<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Defining the output results shape. This is the images' shape which will be fed to the head's detectoin\n",
    "# algorithm later on.\n",
    "max_height = 300\n",
    "max_width = 300\n",
    "\n",
    "# Iterates over directory names (e.g. training01, training02, etc.) in the 'samples' directory to read the images..\n",
    "for directory_name in tqdm(os.listdir('samples')):\n",
    "    # The directories are named like training01, training02, etc.\n",
    "    # To get the directory's ID, we should omit the first 7 letters.\n",
    "    # Then the remaining is basically the ID.\n",
    "    directory_ID = directory_name[8:]\n",
    "    # Iterates over all images in the above mentioned directory.\n",
    "    for sample_file_name in os.listdir(f'samples/{directory_name}'):\n",
    "        # To get the correct annotation data, we need to know the frame_id.\n",
    "        # Images are named like `frame01.jpg`, `frame02.jpg`, and etc.\n",
    "        # To get the frame_id, we should seperate the `frame` word from the file_name.\n",
    "        frame_id = int(sample_file_name.split('.')[0][5:])\n",
    "        \n",
    "#         print('------------------------------------------------------------------')\n",
    "#         print(f'Processing file {directory_name}/{sample_file_name}')\n",
    "        \n",
    "        # Now we can simply define the sample path, and read the image using cv2.\n",
    "        sample_path = f'samples/{directory_name}/{sample_file_name}'\n",
    "        sample_img = cv2.imread(sample_path)\n",
    "        \n",
    "        # After reading the image, we are able to use our `detector` and its `predict(image)` method to compute results.\n",
    "        prediction_results = detector.predict(sample_img)\n",
    "        \n",
    "        # Iterates over results to store the results.\n",
    "        for result in prediction_results:\n",
    "            # Extracting the `boxes` data, which contains the bounding boxes' position.\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            # Checks if the the detector couldn't find any results. In that case it will return the original image.\n",
    "            if len(boxes) > 0:\n",
    "                # Converts the predicted (x, y) position to a list of class `int` since the center supposed to be integers\n",
    "                xyxy = list(map(int, boxes.xyxy[0]))\n",
    "                \n",
    "                # Creates an images by selecting the results' region. \n",
    "                crop_img = sample_img[xyxy[1]: xyxy[3], xyxy[0]: xyxy[2]]\n",
    "                \n",
    "                # Computes the output image's shape.\n",
    "                cropped_img_shape = crop_img.shape\n",
    "                \n",
    "                # Since the data must have the same shape, we are required to crop images in a way\n",
    "                # which contains the Hexbug, and also have a specific shape.\n",
    "                # To do so, we should compute the margins we want.\n",
    "                # `height_margin` and `width_margin` are computed by a simple subtraction of ideal image shape (300, 300)\n",
    "                # from the cropped image.\n",
    "                height_margin = max_height - cropped_img_shape[0]\n",
    "                width_margin = max_width - cropped_img_shape[1]\n",
    "                \n",
    "                # Since the Hexbug can be in corners, in the middle, next to the walls,\n",
    "                # we should check in which direction we can extend the image to reach the specified shape.\n",
    "                \n",
    "                # Checks if the image has enugh bottom-right margin to extend width.\n",
    "                # otherwise, it will extend it from top-left to left.\n",
    "                if xyxy[2] + width_margin > sample_img.shape[1]:\n",
    "                    x1, x2 = xyxy[0] - width_margin, xyxy[2]\n",
    "                else:\n",
    "                    x1, x2 = xyxy[0], xyxy[2] + width_margin\n",
    "\n",
    "                # Checks if the image has enugh bottom-right margin to extend height.\n",
    "                # otherwise, it will extend it from top-left to up.\n",
    "                if xyxy[3] + height_margin > sample_img.shape[0]:\n",
    "                    y1, y2 = xyxy[1] - height_margin, xyxy[3]\n",
    "                else:\n",
    "                    y1, y2 = xyxy[1], xyxy[3] + height_margin\n",
    "                \n",
    "                # After adding margins, we have the margined_crop_image with desired shape.\n",
    "                margined_crop_img = sample_img[y1: y2, x1: x2]\n",
    "                \n",
    "                # Checks if the directory for the cropped image exists.\n",
    "                if not os.path.exists(f'./cropped_bugs/{directory_name}'):\n",
    "                    os.mkdir(f'./cropped_bugs/{directory_name}')\n",
    "                \n",
    "                # Writes the margined_crop_image to the specified directory.\n",
    "                cv2.imwrite(f'./cropped_bugs/{directory_name}/{sample_file_name}', margined_crop_img)\n",
    "                \n",
    "                # Opens the annotations file (traco file) to extract the Hexbugs's head position.\n",
    "                with open(f'./Annotations/training{directory_ID}.traco') as file:\n",
    "                    sample_ann = json.load(file)\n",
    "                \n",
    "                # (x, y) is the center of the hypothetical center, and need to be `int`.\n",
    "                x, y = int(sample_ann['rois'][frame_id]['pos'][0]), int(sample_ann['rois'][frame_id]['pos'][1])\n",
    "                \n",
    "                # New coordinations according to the margined_cropped_image are calculated.\n",
    "                # To do so, basically subtracts the head's original position in the original image\n",
    "                # from the bounding box's coordinations.\n",
    "                new_x, new_y = x - x1, y - y1\n",
    "                \n",
    "                # To insert the data to the dataframe, we create a row with the given fields.\n",
    "                new_row = pd.DataFrame(\n",
    "                    {'CroppedHexBugCoordinationX': new_x,\n",
    "                     'CroppedHexBugCoordinationY': new_y,\n",
    "                     'OriginalBoxCoordinationX1': x1,\n",
    "                     'OriginalBoxCoordinationY1': y1,\n",
    "                     'Path': f'cropped_bugs/{directory_name}/{sample_file_name}',\n",
    "                     'ID': frame_id},\n",
    "                    index=[0])\n",
    "                \n",
    "                # Now we reset the index and concatinate the new row with the dataframe.\n",
    "                data = pd.concat([new_row, data.iloc[:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f134968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the `data.csv` file to the same path, so it can be used later in model training.\n",
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c5113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
